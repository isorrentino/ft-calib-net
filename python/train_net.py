import tensorflow as tf
from tensorflow import keras
import h5py
import numpy as np
from sklearn.model_selection import train_test_split
import pickle


def train_net(file, part):
    dataset = dict()
    with h5py.File(file, "r") as f:
        dataset["ang_vel"] = np.array(f["dataset"][part]["ang_vel"])
        dataset["lin_acc"] = np.array(f["dataset"][part]["lin_acc"])
        dataset["orientation_quat"] = np.array(f["dataset"][part]["orientation_quat"])
        dataset["ft_expected"] = np.array(f["dataset"][part]["ft_expected"])
        dataset["ft_measured"] = np.array(f["dataset"][part]["ft_measured"])

    # scale dataset
    scaling = dict()
    for key, value in dataset.items():
        if key != "orientation_quat":
            scaling[key] = dict()
            scaling[key]["mean"] = np.mean(value, axis=0)
            scaling[key]["std"] = np.std(value, axis=0)
            dataset[key] = (value - scaling[key]["mean"]) / scaling[key]["std"]

    # augment dataset using quaternion property
    numpy_dataset = np.concatenate(
        (
            np.concatenate(
                (
                    dataset["ang_vel"],
                    dataset["lin_acc"],
                    dataset["orientation_quat"],
                    dataset["ft_measured"],
                ),
                axis=1,
            ),
            np.concatenate(
                (
                    dataset["ang_vel"],
                    dataset["lin_acc"],
                    -dataset["orientation_quat"],
                    dataset["ft_measured"],
                ),
                axis=1,
            ),
        ),
        axis=0,
    )
    numpy_expected = np.concatenate(
        (dataset["ft_expected"], dataset["ft_expected"]), axis=0
    )

    train_examples = np.expand_dims(numpy_dataset, axis=2)
    train_labels = np.expand_dims(numpy_expected, axis=2)

    # train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_labels))
    x_train, x_valid, y_train, y_valid = train_test_split(
        train_examples, train_labels, test_size=0.33, shuffle=True
    )

    model = keras.Sequential(
        [keras.layers.Dense(
            input_shape=(6 + 4 + 3 + 3,),
            units=6 + 4,
            activation="relu",
            kernel_regularizer=keras.regularizers.L2(0.01),
            activity_regularizer=keras.regularizers.L2(0.01),
        ),
            # keras.layers.Dense(units=6 + 4,
            #                    activation='relu',
            #                    kernel_regularizer=keras.regularizers.L2(0.01),
            #                    activity_regularizer=keras.regularizers.L2(0.01)),
            keras.layers.Dense(units=6, activation="linear"),
        ]
    )
    model.compile(
        loss=tf.keras.losses.MeanSquaredError(reduction="auto", name="mean_squared_error"),
        metrics=keras.metrics.MeanSquaredError(name="mean_squared_error", dtype=None),
        optimizer="adam",
    )

    model.fit(
        x=x_train, y=y_train, epochs=50, batch_size=150, validation_data=(x_valid, y_valid)
    )

    return model, scaling


if __name__ == "__main__":
    parts = ['l_arm', 'r_arm', 'l_foot_front', 'l_foot_rear', 'r_foot_front', 'r_foot_rear', 'l_leg', 'r_leg']
    scaling = dict()
    models = dict()
    for part in parts:
        models[part], scaling[part] = train_net("../datasets/calib_dataset.mat", part)

    for part in parts:
        models[part].save("../autogenerated/models_" + part + "/")
        models[part].save("../export_model/" + part + " _net.h5", include_optimizer=False)

    with open("../autogenerated/scaling.pickle", "wb") as handle:
        pickle.dump(scaling, handle)
