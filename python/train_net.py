import tensorflow as tf
from tensorflow import keras
import h5py
import numpy as np
from sklearn.model_selection import train_test_split
import pickle


def train_net(file, part):
    dataset = dict()
    with h5py.File(file, "r") as f:
        dataset["ang_vel"] = np.array(f["dataset"][part]["ang_vel"])
        dataset["lin_acc"] = np.array(f["dataset"][part]["lin_acc"])
        dataset["orientation_quat"] = np.array(f["dataset"][part]["orientation_quat"])
        dataset["ft_expected"] = np.array(f["dataset"][part]["ft_expected"])
        dataset["ft_measured"] = np.array(f["dataset"][part]["ft_measured"])

    # scale dataset
    scaling = dict()
    for key, value in dataset.items():
        if key != "orientation_quat":
            scaling[key] = dict()
            scaling[key]["mean"] = np.mean(value, axis=0)
            scaling[key]["std"] = np.std(value, axis=0)
            dataset[key] = (value - scaling[key]["mean"]) / scaling[key]["std"]

    # augment dataset using quaternion property
    numpy_dataset = np.concatenate(
        (
            np.concatenate(
                (
                    # dataset["ang_vel"],
                    # dataset["lin_acc"],
                    # dataset["orientation_quat"],
                    dataset["ft_measured"],
                ),
                axis=1,
            ),
            np.concatenate(
                (
                    # dataset["ang_vel"],
                    # dataset["lin_acc"],
                    # -dataset["orientation_quat"],
                    dataset["ft_measured"],
                ),
                axis=1,
            ),
        ),
        axis=0,
    )
    numpy_expected = np.concatenate(
        (dataset["ft_expected"], dataset["ft_expected"]), axis=0
    )

    train_examples = np.expand_dims(numpy_dataset, axis=2)
    train_labels = np.expand_dims(numpy_expected, axis=2)

    # train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_labels))
    x_train, x_valid, y_train, y_valid = train_test_split(
        train_examples, train_labels, test_size=0.33, shuffle=True
    )

    model = keras.Sequential(
        [keras.layers.Dense(
            input_shape=(6,),
            units=50,
            activation="relu",
            # kernel_initializer="uniform",
            # kernel_regularizer=keras.regularizers.L2(0.01),
            # activity_regularizer=keras.regularizers.L2(0.01),
            name="layer1"
        ),
            # keras.layers.Dropout(rate=0.1,
            #                    # kernel_regularizer=keras.regularizers.L2(0.01),
            #                    # activity_regularizer=keras.regularizers.L2(0.01),
            #                    name="layer2"),
            keras.layers.Dense(units=30,
                               # kernel_regularizer=keras.regularizers.L2(0.01),
                               # activity_regularizer=keras.regularizers.L2(0.01),
                               name="layer2"),
            # keras.layers.Dropout(rate=0.3,
            #                      # kernel_regularizer=keras.regularizers.L2(0.01),
            #                      # activity_regularizer=keras.regularizers.L2(0.01),
            #                      name="layer3"),
            keras.layers.Dense(units=15,
                               # kernel_regularizer=keras.regularizers.L2(0.01),
                               # activity_regularizer=keras.regularizers.L2(0.01),
                               name="layer4"),
            keras.layers.Dense(units=6,
                               # kernel_regularizer=keras.regularizers.L2(0.01),
                               # activity_regularizer=keras.regularizers.L2(0.01),
                               name="layer5"),
            # keras.layers.Dense(units=6),
        ]
    )

    model.compile(
        loss=tf.keras.losses.MeanSquaredError(reduction="auto", name="mean_squared_error"),
        metrics=keras.metrics.MeanSquaredError(name="mean_squared_error", dtype=None),
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    )

    model.fit(
        x=x_train, y=y_train, epochs=10, batch_size=60, validation_data=(x_valid, y_valid)
    )

    return model, scaling


if __name__ == "__main__":
    parts = ['r_arm']
    scaling = dict()
    models = dict()
    for part in parts:
        models[part], scaling[part] = train_net("../datasets/calib_dataset_3.mat", part)

    for part in parts:
        models[part].save("../autogenerated/models_" + part + "/")
        models[part].save("../export_model/" + part + "_net.h5", include_optimizer=False)

    with open("../autogenerated/scaling.pickle", "wb") as handle:
        pickle.dump(scaling, handle)
